{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Return Prediction\n",
    "\n",
    "In this case study we will use various supervised learning-based models to predict the\n",
    "stock price of Microsoft using correlated assets and its own historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Libraries and Dataset](#1)\n",
    "    * [2.1. Load Libraries](#1.1)    \n",
    "    * [2.2. Load Dataset](#1.2)\n",
    "* [3. Exploratory Data Analysis](#2)\n",
    "    * [3.1 Descriptive Statistics](#2.1)    \n",
    "    * [3.2. Data Visualisation](#2.2)\n",
    "* [4. Data Preparation](#3)\n",
    "    * [4.1 Data Cleaning](#3.1)    \n",
    "    * [4.2.Feature Selection](#3.2) \n",
    "* [5.Evaluate Algorithms and Models](#4)        \n",
    "    * [5.1. Train/Test Split](#4.1)\n",
    "    * [5.2. Evaluation Metrics](#4.2)\n",
    "    * [5.3. Compare Models and Algorithms](#4.3)\n",
    "        * [5.3.1 Machine Learning models-scikit-learn](#4.3.1)\n",
    "        * [5.3.2 Time Series based Models-ARIMA and LSTM](#4.3.2)\n",
    "* [6. Model Tuning and grid search](#5)\n",
    "* [7. Finalise the model](#6)\n",
    "    * [7.1. Result on the test dataset](#6.1)\n",
    "    * [7.2. Save Model for Later Use](#6.2)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the supervised regression framework used for this case study, weekly return of the\n",
    "Microsoft stock is the predicted variable. We need to understand what affects Microsoft stock price and hence incorporate as much information into the model. \n",
    "\n",
    "For this case study, other than the historical data of Microsoft, the independent variables used are the following potentially correlated assets:\n",
    "* Stocks: IBM (IBM) and Alphabet (GOOGL)\n",
    "* Currency: USD/JPY and GBP/USD\n",
    "* Indices: S&P 500, Dow Jones and VIX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# 2. Getting Started- Loading the data and python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Loading the python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import yfinance as yf\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#Libraries for Deep Learning Models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import LSTM\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Libraries for Saving the Model\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "# Time series Models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "#from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Error Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_regression\n",
    "\n",
    "\n",
    "#Plotting \n",
    "from pandas.plotting import scatter_matrix\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "## 2.2. Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the data required for our analysis using pandas datareader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk_tickers = ['MSFT', 'IBM', 'GOOGL']\n",
    "ccy_tickers = ['DEXJPUS', 'DEXUSUK']\n",
    "idx_tickers = ['SP500', 'DJIA', 'VIXCLS']\n",
    "\n",
    "stk_data = yf.download(stk_tickers, auto_adjust=False)\n",
    "# stk_data = web.DataReader(stk_tickers, 'google')\n",
    "ccy_data = web.DataReader(ccy_tickers, 'fred')\n",
    "idx_data = web.DataReader(idx_tickers, 'fred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a series to predict. We choose to predict using weekly returns. We approximate this by using 5 business day period returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_period = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our Y series and our X series\n",
    "\n",
    "Y: MSFT **Future** Returns\n",
    "\n",
    "X:\n",
    "\n",
    "    a. GOOGL 5 Business Day Returns\n",
    "    b. IBM 5 Business DayReturns    \n",
    "    c. USD/JPY 5 Business DayReturns    \n",
    "    d. GBP/USD 5 Business DayReturns    \n",
    "    e. S&P 500 5 Business DayReturns    \n",
    "    f. Dow Jones 5 Business DayReturns    \n",
    "    g. MSFT 5 Business Day Returns    \n",
    "    h. MSFT 15 Business Day Returns    \n",
    "    i. MSFT 30 Business Day Returns    \n",
    "    j. MSFT 60 Business Day Returns\n",
    "\n",
    "We remove the MSFT past returns when we use the Time series models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.log(stk_data.loc[:, ('Adj Close', 'MSFT')]).diff(return_period).shift(-return_period)\n",
    "Y.name = Y.name[-1]+'_pred'\n",
    "\n",
    "X1 = np.log(stk_data.loc[:, ('Adj Close', ('GOOGL', 'IBM'))]).diff(return_period)\n",
    "X1.columns = X1.columns.droplevel()\n",
    "X2 = np.log(ccy_data).diff(return_period)\n",
    "X3 = np.log(idx_data).diff(return_period)\n",
    "\n",
    "X4 = pd.concat([np.log(stk_data.loc[:, ('Adj Close', 'MSFT')]).diff(i) for i in [return_period, return_period*3, return_period*6, return_period*12]], axis=1).dropna()\n",
    "X4.columns = ['MSFT_DT', 'MSFT_3DT', 'MSFT_6DT', 'MSFT_12DT']\n",
    "\n",
    "X = pd.concat([X1, X2, X3, X4], axis=1)\n",
    "\n",
    "dataset = pd.concat([Y, X], axis=1).dropna().iloc[::return_period, :]\n",
    "Y = dataset.loc[:, Y.name]\n",
    "X = dataset.loc[:, X.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "## 3.1. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the dataset we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 3)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "## 3.2. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets look at the distribution of the data over the entire period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(bins=50, sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(12,12))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram shows the distribution for each series individually. Next, lets look at the density distribution over the same x axis scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='density', subplots=True, layout=(4,4), sharex=True, legend=True, fontsize=1, figsize=(15,15))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the vix has a much larger variance compared to the other distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of the interdependence of the data we look at the scatter plot and the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = dataset.corr()\n",
    "pyplot.figure(figsize=(15,15))\n",
    "pyplot.title('Correlation Matrix')\n",
    "sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the correlation plot above, we see some correlation of the predicted vari窶申n",
    "able with the lagged 5 days, 15days, 30 days and 60 days return of MSFT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(15,15))\n",
    "scatter_matrix(dataset,figsize=(12,12))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the scatter plot above, we see some linear relationship of the predicted\n",
    "variable the lagged 15 days, 30 days and 60 days return of MSFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.3'></a>\n",
    "## 3.3. Time Series Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the seasonal decomposition of our time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sm.tsa.seasonal_decompose(Y, period=52)\n",
    "fig = res.plot()\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(15)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for MSFT there has been a general trend upwards. This should show up in our the constant/bias terms in our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "## 4.2. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use sklearn's SelectKBest function to get a sense of feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures = SelectKBest(k=5, score_func=f_regression)\n",
    "fit = bestfeatures.fit(X,Y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "featureScores.nlargest(10,'Score').set_index('Specs')  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that IBM seems to be the most important feature and vix being the least important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# 5. Evaluate Algorithms and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "## 5.1. Train Test Split and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start by splitting our data in training and testing chunks. If we are going to use Time series models we have to split the data in continous series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.2\n",
    "\n",
    "#In case the data is not dependent on the time series, then train and test split randomly\n",
    "# seed = 7\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample\n",
    "#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.\n",
    "train_size = int(len(X) * (1-validation_size))\n",
    "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
    "Y_train, Y_test = Y[0:train_size], Y[train_size:len(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "## 5.2. Test Options and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 7\n",
    "# scikit is moving away from mean_squared_error. \n",
    "# In order to avoid confusion, and to allow comparison with other models, we invert the final scores\n",
    "scoring = 'neg_mean_squared_error' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>\n",
    "## 5.3. Compare Models and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3.1'></a>\n",
    "### 5.3.1 Machine Learning models-from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression and Tree Regression algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('LASSO', Lasso()))\n",
    "models.append(('EN', ElasticNet()))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor()))\n",
    "models.append(('SVR', SVR()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(('MLP', MLPRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensable Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting methods\n",
    "models.append(('ABR', AdaBoostRegressor()))\n",
    "models.append(('GBR', GradientBoostingRegressor()))\n",
    "# Bagging methods\n",
    "models.append(('RFR', RandomForestRegressor()))\n",
    "models.append(('ETR', ExtraTreesRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have selected all the models, we loop over each of them. First we run the K-fold analysis. Next we run the model on the entire training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "kfold_results = []\n",
    "test_results = []\n",
    "train_results = []\n",
    "for name, model in models:\n",
    "    names.append(name)\n",
    "    \n",
    "    ## K Fold analysis:\n",
    "    \n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    #converted mean square error to positive. The lower the beter\n",
    "    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    kfold_results.append(cv_results)\n",
    "    \n",
    "\n",
    "    # Full Training period\n",
    "    res = model.fit(X_train, Y_train)\n",
    "    train_result = mean_squared_error(res.predict(X_train), Y_train)\n",
    "    train_results.append(train_result)\n",
    "    \n",
    "    # Test results\n",
    "    test_result = mean_squared_error(res.predict(X_test), Y_test)\n",
    "    test_results.append(test_result)\n",
    "    \n",
    "    msg = \"%s: %f (%f) %f %f\" % (name, cv_results.mean(), cv_results.std(), train_result, test_result)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Fold results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We being by looking at the K Fold results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison: Kfold results')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(kfold_results)\n",
    "ax.set_xticklabels(names)\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the linear regression and the regularized regression including the Lasso regression (LASSO) and elastic net (EN) seem to do a good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare algorithms\n",
    "fig = pyplot.figure()\n",
    "\n",
    "ind = np.arange(len(names))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.bar(ind - width/2, train_results,  width=width, label='Train Error')\n",
    "pyplot.bar(ind + width/2, test_results, width=width, label='Test Error')\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.legend()\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training and test error, we still see a better performance of the linear models. Some of the algorithms, such as the decision tree regressor (CART) overfit on\n",
    "the training data and produced very high error on the test set and these models\n",
    "should be avoided. Ensemble models, such as gradient boosting regression (GBR) and\n",
    "random forest regression (RFR) have low bias but high variance. We also see that the\n",
    "artificial neural network (shown as MLP is the chart) algorithm shows higher errors\n",
    "both in training set and test set, which is perhaps due to the linear relationship of the\n",
    "variables not captured accurately by ANN or improper hyperparameters or insuffi窶申n",
    "cient training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3.1'></a>\n",
    "### 5.3.1 Time Series based models-ARIMA and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first prepare the dataset for ARIMA models,\n",
    "by having only the correlated varriables as exogenous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Model - ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ARIMA=X_train.loc[:, ['GOOGL', 'IBM', 'DEXJPUS', 'SP500', 'DJIA', 'VIXCLS']]\n",
    "X_test_ARIMA=X_test.loc[:, ['GOOGL', 'IBM', 'DEXJPUS', 'SP500', 'DJIA', 'VIXCLS']]\n",
    "tr_len = len(X_train_ARIMA)\n",
    "te_len = len(X_test_ARIMA)\n",
    "to_len = len (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=(1, 0, 0))\n",
    "model_fit = modelARIMA.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_Training_ARIMA = mean_squared_error(Y_train, model_fit.fittedvalues)\n",
    "predicted = model_fit.predict(start = tr_len -1 ,end = to_len -1, exog = X_test_ARIMA)[1:]\n",
    "error_Test_ARIMA = mean_squared_error(Y_test,predicted)\n",
    "error_Test_ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 2 #Length of the seq for the LSTM\n",
    "\n",
    "Y_train_LSTM, Y_test_LSTM = np.array(Y_train)[seq_len-1:], np.array(Y_test)\n",
    "X_train_LSTM = np.zeros((X_train.shape[0]+1-seq_len, seq_len, X_train.shape[1]))\n",
    "X_test_LSTM = np.zeros((X_test.shape[0], seq_len, X.shape[1]))\n",
    "for i in range(seq_len):\n",
    "    X_train_LSTM[:, i, :] = np.array(X_train)[i:X_train.shape[0]+i+1-seq_len, :]\n",
    "    X_test_LSTM[:, i, :] = np.array(X)[X_train.shape[0]+i-1:X.shape[0]+i+1-seq_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lstm Network\n",
    "def create_LSTMmodel(neurons=12, learn_rate = 0.01, momentum=0):\n",
    "        # create model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2])))\n",
    "    #More number of cells can be added if needed \n",
    "    model.add(Dense(1))\n",
    "    # optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "LSTMModel = create_LSTMmodel(12, learn_rate = 0.01, momentum=0)\n",
    "LSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM, validation_data=(X_test_LSTM, Y_test_LSTM),epochs=330, batch_size=72, verbose=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual plot to check if the error is reducing\n",
    "pyplot.plot(LSTMModel_fit.history['loss'], label='train')\n",
    "pyplot.plot(LSTMModel_fit.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_Training_LSTM = mean_squared_error(Y_train_LSTM, LSTMModel.predict(X_train_LSTM))\n",
    "predicted = LSTMModel.predict(X_test_LSTM)\n",
    "error_Test_LSTM = mean_squared_error(Y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append to previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.append(error_Test_ARIMA)\n",
    "test_results.append(error_Test_LSTM)\n",
    "\n",
    "train_results.append(error_Training_ARIMA)\n",
    "train_results.append(error_Training_LSTM)\n",
    "\n",
    "names.append(\"ARIMA\")\n",
    "names.append(\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Comparison of all the algorithms ( including Time Series Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare algorithms\n",
    "fig = pyplot.figure()\n",
    "\n",
    "ind = np.arange(len(names))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig.suptitle('Comparing the performance of various algorthims on the Train and Test Dataset')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.bar(ind - width/2, train_results,  width=width, label='Train Error')\n",
    "pyplot.bar(ind + width/2, test_results, width=width, label='Test Error')\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.legend()\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.ylabel('Mean Square Error')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the chart above, we find time series based ARIMA model comparable to\n",
    "the linear supervised-regression models such as Linear Regression (LR), Lasso Regres窶申n",
    "sion (LASSO) and Elastic Net (EN). This can primarily be due to the strong linear\n",
    "relationship as discussed before. The LSTM model performs decently, however,\n",
    "ARIMA model outperforms the LSTM model in the test set. Hence, we select the\n",
    "ARIMA model for the model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# 6. Model Tuning and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the chart above the ARIMA model is one of the best mode, so we perform the model tuning of the ARIMA model. The default order of ARIMA model is [1,0,0]. We perform a grid search with different combination p,d and q in the ARIMA model's order.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for ARIMA Model\n",
    "#Change p,d and q and check for the best result\n",
    "\n",
    "# evaluate an ARIMA model for a given order (p,d,q)\n",
    "#Assuming that the train and Test Data is already defined before\n",
    "def evaluate_arima_model(arima_order):\n",
    "    #predicted = list()     \n",
    "    modelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=arima_order)\n",
    "    model_fit = modelARIMA.fit()\n",
    "    error = mean_squared_error(Y_train, model_fit.fittedvalues)\n",
    "    return error\n",
    " \n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(p_values, d_values, q_values): \n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)                \n",
    "                try:\n",
    "                    mse = evaluate_arima_model(order)                    \n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.7f' % (order,mse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s MSE=%.7f' % (best_cfg, best_score))\n",
    "    \n",
    "# evaluate parameters\n",
    "p_values = [0, 1, 2]\n",
    "d_values = range(0, 2)\n",
    "q_values = range(0, 2)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(p_values, d_values, q_values)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "# 7. Finalise the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1. Results on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "modelARIMA_tuned=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[2,0,1])\n",
    "model_fit_tuned = modelARIMA_tuned.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate accuracy on validation set\n",
    "predicted_tuned = model_fit.predict(start = tr_len -1 ,end = to_len -1, exog = X_test_ARIMA)[1:]\n",
    "print(mean_squared_error(Y_test,predicted_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the model and picking the best ARIMA model or the order 2,0 and 1 we select this model and can it can be used for the modeling purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2. Save Model for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model Using Pickle\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "dump(model_fit_tuned, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the following code to produce the comparison of actual vs. predicted\n",
    "predicted_tuned.index = Y_test.index\n",
    "pyplot.plot(np.exp(Y_test).cumprod(), 'r') # plotting t, a separately\n",
    "pyplot.plot(np.exp(predicted_tuned).cumprod(), 'b')\n",
    "pyplot.rcParams[\"figure.figsize\"] = (8,5)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that simple models - linear regression, regularized regression (i.e.\n",
    "Lasso and elastic net) - along with the time series model such as ARIMA are promis窶申n",
    "ing modelling approaches for asset price prediction problem. These models can\n",
    "enable financial practitioners to model time dependencies with a very flexible\n",
    "approach. The overall approach presented in this case study may help us encounter\n",
    "overfitting and underfitting which are some of the key challenges in the prediction\n",
    "problem in finance.\n",
    "We should also note that we can use better set of indicators, such as P/E ratio, trading\n",
    "volume, technical indicators or news data, which might lead to better results. We will\n",
    "demonstrate this in some of the case studies in the book.\n",
    "Overall, we created a supervised-regression and time series modelling framework\n",
    "which allows us to perform asset class prediction using historical data to generate\n",
    "results and analyze risk and profitability before risking any actual capital."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
